{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_list contains the cik of all public US firms\n",
    "import csv\n",
    "with open(\"C:/Users/25280/Downloads/myproject/cleaned_list.csv\", 'w', newline='', encoding = \"utf-8\") as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow(cleaned_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c642023",
   "metadata": {},
   "source": [
    "Main program for extracting exhibits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54cafc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import html\n",
    "from urllib.parse import urlparse\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration\n",
    "# ---------------------------\n",
    "HEADERS = {\"User-Agent\": \"ResearchBot/1.0 (your_email@example.com)\"}\n",
    "SEC_BASE = \"https://www.sec.gov/Archives/edgar/full-index\"\n",
    "DOWNLOAD_ROOT = \"C://Users//25280//Downloads//myproject//ex10_downloads\"\n",
    "FORMS = {\"10-K\", \"10-Q\", \"8-K\"}\n",
    "MAX_WORKERS = 12\n",
    "#KEYWORDS = [\n",
    "#    \"cybersecurity\", \"cyber risk\", \"data breach\", \"information security\",\n",
    "#    \"network intrusion\", \"hacker\", \"malware\", \"ransomware\", \"phishing\",\n",
    "#    \"incident response\", \"security controls\", \"vulnerability\"\n",
    "#]\n",
    "OUTPUT_CSV = \"cyber_clauses_ex10.csv\"\n",
    "\n",
    "CYBER_KEYWORDS = [\n",
    "    \"cyber\", \"data breach\", \"information security\", \"network intrusion\",\n",
    "    \"data protection\", \"information systems\", \"data privacy\",\n",
    "    \"personal data\", \"privacy\", \"data processing\", \"security incident\",\n",
    "    \"private data\", \"information technology security\", \"security breach\", \"data restoration\",\n",
    "    \" SOC \", \"IT asset\", \"Privacy Legal Requirements\", \"unauthorized access\"\n",
    "]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Global session\n",
    "# ---------------------------\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def robust_get_text(url):\n",
    "    try:\n",
    "        r = session.get(url, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def fetch_master_idx(year, quarter):\n",
    "    url = f\"{SEC_BASE}/{year}/QTR{quarter}/master.idx\"\n",
    "    return robust_get_text(url)\n",
    "\n",
    "def parse_master_idx_text(idx_text):\n",
    "    lines = idx_text.splitlines()\n",
    "    header_idx = next((i for i, ln in enumerate(lines[:80])\n",
    "                       if ln.strip().startswith(\"CIK|Company Name|Form Type|Date Filed|Filename\")), None)\n",
    "    if header_idx is None:\n",
    "        return pd.DataFrame(columns=[\"cik\",\"company\",\"form\",\"date\",\"filename\"])\n",
    "    data = \"\\n\".join(lines[header_idx+1:])\n",
    "    df = pd.read_csv(StringIO(data), sep=\"|\", header=None,\n",
    "                     names=[\"cik\",\"company\",\"form\",\"date\",\"filename\"],\n",
    "                     dtype=str, engine=\"python\")\n",
    "    df[\"cik\"] = df[\"cik\"].str.strip().str.zfill(10)\n",
    "    df[\"company\"] = df[\"company\"].str.strip()\n",
    "    df[\"form\"] = df[\"form\"].str.strip()\n",
    "    df[\"filename\"] = df[\"filename\"].str.strip()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    return df[[\"cik\",\"company\",\"form\",\"date\",\"filename\"]]\n",
    "\n",
    "def filing_index_headers_url(filename):\n",
    "    parts = filename.split(\"/\")\n",
    "    cik = parts[2]\n",
    "    accession_file = parts[-1].replace(\".txt\",\"\")\n",
    "    accession = \"\".join(accession_file.split(\"-\"))\n",
    "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession}/{accession_file}-index-headers.html\"\n",
    "\n",
    "def find_ex10_links(url_index):\n",
    "    html = robust_get_text(url_index)\n",
    "    if not html:\n",
    "        return []\n",
    "    EX10 = re.compile(r\"ex[-_\\.]?\\s*10[\\w\\.\\-_]*\\.(htm|html)\", re.IGNORECASE)\n",
    "    base = url_index.rsplit(\"/\", 1)[0]\n",
    "    links = []\n",
    "    for match in re.finditer(r'href=\"([^\"]*ex[-_\\.]?\\s*10[\\w\\.\\-_]*\\.(htm|html))\"', html, flags=re.IGNORECASE):\n",
    "        h = match.group(1)\n",
    "        full_url = h if h.startswith(\"http\") else base + \"/\" + h\n",
    "        links.append(full_url)\n",
    "    return list(dict.fromkeys(links))\n",
    "\n",
    "def download_exhibit(url, cik, company):\n",
    "    safe_name = \"\".join(c for c in company if c.isalnum() or c in (\" \", \"_\")).strip().replace(\" \", \"_\")\n",
    "    firm_folder = os.path.join(DOWNLOAD_ROOT, f\"{cik}_{safe_name}\")\n",
    "    os.makedirs(firm_folder, exist_ok=True)\n",
    "    path = os.path.normpath(urlparse(url).path)  # normalize slashes\n",
    "    parts = path.split(os.sep)\n",
    "    fname = parts[-2] + \"+\" + parts[-1]\n",
    "    #fname = os.path.basename(urlparse(url).path).split(\"?\")[0]\n",
    "    save_path = os.path.join(firm_folder, fname)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    txt = robust_get_text(url)\n",
    "    if not txt:\n",
    "        return None\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        f.write(txt)\n",
    "    return save_path\n",
    "\n",
    "def extract_clean_text_from_html(html_path):\n",
    "    \"\"\"Extract all paragraph text cleanly from the HTML file.\"\"\"\n",
    "    with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html = f.read()\n",
    "\n",
    "        # find every <p>...</p> block exactly as written\n",
    "        p_blocks = re.findall(r'<p[^>]*>(.*?)</p>', html, flags=re.I | re.S)\n",
    "\n",
    "        clean_paragraphs = []\n",
    "        for block in p_blocks:\n",
    "           # remove all tags inside\n",
    "            text = re.sub(r'<[^>]+>', '', block)\n",
    "            text = text.replace('\\xa0', ' ')\n",
    "            text = text.replace('&nbsp', ' ')\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            if text and len(text) > 20:\n",
    "                clean_paragraphs.append(text)\n",
    "    return clean_paragraphs\n",
    "\n",
    "def find_cyber_clauses(paragraphs):\n",
    "    \"\"\"\n",
    "    Extract all cyber-related clauses:\n",
    "    - Captures full 'Cybersecurity and Data Privacy' sections (excluding the heading)\n",
    "    - Splits subclauses (a), (b), (c)... into separate items\n",
    "    - Also picks up isolated cyber paragraphs outside sections\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    i = 0\n",
    "    for i in range(len(paragraphs)):\n",
    "        para = paragraphs[i].strip()\n",
    "        lower_para = para.lower()\n",
    "        # --- 3. Detect isolated cyber-related paragraphs ---\n",
    "        if any(kw in lower_para for kw in CYBER_KEYWORDS):\n",
    "            results.append(para)\n",
    "    # Final cleanup: remove duplicates and junk\n",
    "    cleaned = []\n",
    "    for clause in results:\n",
    "        clause = re.sub(r'\\s+', ' ', clause).strip()\n",
    "        if len(clause) > 80:  # discard trivial lines\n",
    "            cleaned.append(clause)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_contract_type(html_path):\n",
    "    \"\"\"\n",
    "    Reads the top part of an SEC EX-10 exhibit file and tries to extract\n",
    "    the contract title (e.g., 'Credit Agreement', 'Purchase Agreement', etc.)\n",
    "    purely from plain text, without HTML parsing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            # Read top 150 KB for safety\n",
    "            raw = f.read(150000)\n",
    "\n",
    "        # Remove HTML tags and unescape entities\n",
    "        text = re.sub(r\"<[^>]+>\", \" \", raw)\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Split into candidate \"lines\" based on typical breaks\n",
    "        lines = re.split(r\"(?:\\s{2,}|[\\r\\n]+|<br>|<br/>|<BR>)\", raw)\n",
    "        candidates = []\n",
    "\n",
    "        for line in lines[:300]:  # only first ~300 fragments\n",
    "            clean = re.sub(r\"<[^>]+>\", \" \", line)\n",
    "            clean = html.unescape(clean)\n",
    "            clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
    "            if not clean:\n",
    "                continue\n",
    "            # must contain \"agreement\"\n",
    "            if re.search(r\"\\bagreement\\b\", clean, re.I):\n",
    "                # ignore money lines or exhibit references\n",
    "                if re.search(r\"\\$\\s*\\d\", clean) or re.search(r\"exhibit\\s*10\", clean, re.I):\n",
    "                    continue\n",
    "                if len(clean.split(\" \")) > 1:\n",
    "                # looks like a valid title\n",
    "                    candidates.append(clean)\n",
    "        \n",
    "        #print(candidates[:5])\n",
    "\n",
    "        if not candidates:\n",
    "            return \"No Type\"\n",
    "\n",
    "        # Choose the shortest plausible line (most likely the true title)\n",
    "        title = min(candidates, key=len)\n",
    "        # Basic cleanup\n",
    "        title = re.sub(r\"^\\W+|\\W+$\", \"\", title)\n",
    "        title = re.sub(r\"\\s{2,}\", \" \", title)\n",
    "        return title.strip()\n",
    "\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main processing\n",
    "# ---------------------------\n",
    "def process_cik(cik, start_year, end_year):\n",
    "    cik_z = str(cik).zfill(10)\n",
    "\n",
    "    # load previous output CSV if exists to skip reprocessing\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        existing_df = pd.read_csv(OUTPUT_CSV, encoding=\"utf-8-sig\")\n",
    "        processed_files = set(existing_df[\"htm_file\"].tolist())\n",
    "    else:\n",
    "        processed_files = set()\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    frames = []\n",
    "    for year in range(start_year, end_year+1):\n",
    "        for q in range(1,5):\n",
    "            idx = fetch_master_idx(year, q)\n",
    "            if not idx:\n",
    "                continue\n",
    "            df = parse_master_idx_text(idx)\n",
    "            if df.empty:\n",
    "                continue\n",
    "            sel = df[(df[\"cik\"] == cik_z) & (df[\"form\"].isin(FORMS))]\n",
    "            if not sel.empty:\n",
    "                # filter by year\n",
    "                sel = sel[sel[\"date\"].dt.year.between(start_year, end_year)]\n",
    "                if not sel.empty:\n",
    "                    frames.append(sel)\n",
    "    if not frames:\n",
    "        #print(f\"[{cik_z}] no filings found.\")\n",
    "        return\n",
    "\n",
    "    filings_df = pd.concat(frames, ignore_index=True)\n",
    "    filings_df[\"index_url\"] = filings_df[\"filename\"].apply(filing_index_headers_url)\n",
    "\n",
    "    # fetch EX-10 links concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        ex10_lists = list(executor.map(find_ex10_links, filings_df[\"index_url\"]))\n",
    "    \n",
    "    #print(ex10_lists)\n",
    "    download_tasks = []\n",
    "    for i, links in enumerate(ex10_lists):\n",
    "        company = filings_df.iloc[i][\"company\"]\n",
    "        filing_date = filings_df.iloc[i][\"date\"]\n",
    "        for url in links:\n",
    "            safe_name = \"\".join(c for c in company if c.isalnum() or c in (\" \", \"_\")).strip().replace(\" \", \"_\")\n",
    "            firm_folder = os.path.join(DOWNLOAD_ROOT, f\"{cik_z}_{safe_name}\")\n",
    "            #print(urlparse(url).path)\n",
    "            path = os.path.normpath(urlparse(url).path)  # normalize slashes\n",
    "            parts = path.split(os.sep)\n",
    "            fname = parts[-2] + \"+\" + parts[-1]\n",
    "            #fname = os.path.basename(urlparse(url).path).split(\"?\")[0]\n",
    "            save_path = os.path.join(firm_folder, fname)\n",
    "            #print(save_path)\n",
    "            if save_path in processed_files:\n",
    "                continue\n",
    "            download_tasks.append((url, cik_z, company, filing_date))\n",
    "\n",
    "    extracted = []\n",
    "    def handle_task(task):\n",
    "        url, cik_z, company, filing_date = task\n",
    "        htm_path = download_exhibit(url, cik_z, company)\n",
    "        if not htm_path:\n",
    "            return []\n",
    "        #print(htm_path)\n",
    "        paragraphs = extract_clean_text_from_html(htm_path)\n",
    "        clauses = find_cyber_clauses(paragraphs)\n",
    "        #print(len(clauses))\n",
    "        contract_type = extract_contract_type(htm_path)\n",
    "        rows = []\n",
    "        for clause in clauses:\n",
    "            rows.append({\n",
    "                \"cik\": cik_z,\n",
    "                \"company\": company,\n",
    "                \"filing_date\": filing_date,\n",
    "                \"htm_file\": htm_path,\n",
    "                \"contract_type\": contract_type,\n",
    "                \"clause\": clause\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        for rows in executor.map(handle_task, download_tasks):\n",
    "            if rows:\n",
    "                extracted.extend(rows)\n",
    "\n",
    "    # append new results to CSV\n",
    "    if extracted:\n",
    "        df_new = pd.DataFrame(extracted)\n",
    "        df_out = pd.concat([existing_df, df_new], ignore_index=True)\n",
    "        df_out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[{cik_z}] Saved {len(df_new)} new clauses, total CSV rows: {len(df_out)}\")\n",
    "    #else:\n",
    "        #print(f\"[{cik_z}] No new cyber clauses found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6252e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['STYLE=\"white-space:nowrap\">364-DAY SENIOR UNSECURED TERM LOAN CREDIT AGREEMENT dated as of']\n",
      "['PURCHASE AND SALE AGREEMENT']\n",
      "['STYLE=\"white-space:nowrap\">364-DAY SENIOR UNSECURED TERM LOAN CREDIT AGREEMENT dated as of']\n",
      "[0001047862] Saved 7 new clauses, total CSV rows: 8\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    target_ciks = [\"1047862\"]  # sample firm\n",
    "    process_cik(\"1047862\", 2021, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# ---------------------------\n",
    "# Run for year 2024\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.perf_counter()\n",
    "    for i in range(1211, len(cleaned_list)):\n",
    "        TARGET_CIK = cleaned_list[i]\n",
    "        process_cik(TARGET_CIK, 2024, 2024)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"代码块执行时间为: {execution_time} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17218c9b",
   "metadata": {},
   "source": [
    "Statistics for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ea3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15638\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Root folder where you want to search\n",
    "root_dir = r\"C:\\Users\\25280\\Downloads\\myproject\\ex10_downloads\"\n",
    "\n",
    "# Directory where you want to save matching files\n",
    "output_dir = r\"D:\\EDGAR\\ex10_downloads\\2024\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Target creation time\n",
    "target_time1 = datetime(2025, 11, 10, 4, 0)\n",
    "target_time2 = datetime(2025, 11, 10, 21, 0)\n",
    "\n",
    "# Time tolerance (seconds). You can set to 0 for exact match.\n",
    "time_tolerance = 1  # e.g., within 1 second\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        # Get creation time (Windows-specific)\n",
    "        ctime = os.path.getctime(file_path)\n",
    "        file_creation_time = datetime.fromtimestamp(ctime)\n",
    "\n",
    "        # Filter by creation time (exact or within tolerance)\n",
    "        #if (file_creation_time - target_time1).total_seconds() >= time_tolerance and (file_creation_time - target_time2).total_seconds() <= time_tolerance:\n",
    "        if (file_creation_time - target_time2).total_seconds() >= time_tolerance:\n",
    "            count += 1\n",
    "            rel_path = os.path.relpath(root, root_dir)\n",
    "            firm_folder = rel_path.split(os.sep)[0]  # e.g., \"0001552198_WhiteHorse_Finance_Inc\"\n",
    "\n",
    "            # Prepare destination folder\n",
    "            dest_dir = os.path.join(output_dir, firm_folder)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "            # Copy file\n",
    "            dest_path = os.path.join(dest_dir, file)\n",
    "            shutil.copy2(file_path, dest_path)\n",
    "print(count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
